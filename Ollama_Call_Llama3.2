/* 
-----|------------|-------------------------------------------------
Tipo | Escenario  | Mensaje devuelto
-----|------------|-------------------------------------------------
Err  | Conexión   | Error: No se pudo conectar con la API.
Err  | JSON       | Error al interpretar la respuesta JSON.
Err  | Sin campo  | Campo 'response' no encontrado.
OK   | Exitoso    | Respuesta del modelo, texto limpio y sin errores.
-----|------------|-------------------------------------------------
*/
let
    // Se define una función llamada CallLLMAPI que toma un parámetro de texto llamado 'prompt'
    CallLLMAPI = ( prompt as text ) =>
    let
        // URL del endpoint de la API local para el modelo LLM (corregida)
        url = "http://localhost:11434/api/generate",

        // Se construye el cuerpo de la solicitud en formato JSON, escapando comillas dentro del prompt
        requestBody = "{""model"": ""llama3.2:latest"", ""prompt"": """ & 
                        Text.Replace(prompt, """", "\""") & 
                        """, ""stream"": false}",

        // Intentamos realizar la llamada HTTP POST a la API
        SafeSource = try Web.Contents(
            url,
            [
                Headers = [
                    #"Content-Type" = "application/json"
                ],
                Content = Text.ToBinary(requestBody),
                ManualStatusHandling = {200..299}
            ]
        ) otherwise null,

        // Verificamos si hubo error en la llamada
        ResponseBinary = if SafeSource <> null then Binary.Buffer(SafeSource) else null,

        // Convertimos la respuesta binaria a texto si no es null
        ResponseText = if ResponseBinary <> null then Text.FromBinary(ResponseBinary, TextEncoding.Utf8) else "Error: No se pudo conectar con la API.",

        // Intentamos parsear el JSON, si fue posible recibir texto válido
        SafeJson = try Json.Document(ResponseText) otherwise [response = "Error al interpretar la respuesta JSON."],

        // Extraemos el campo 'response' del JSON
        RawResponse = Record.FieldOrDefault(SafeJson, "response", "Campo 'response' no encontrado."),

        // Limpiamos espacios en blanco
        CleanResponse = Text.Trim(RawResponse)
    in
    CleanResponse,

    // Documentación de la función
    Documentacion = type function (prompt as text) as text meta [
        Documentation.Name = "Call_LLM_API",
        Documentation.LongDescription = 
        "La función Call_LLM_API permite enviar un 'prompt' en texto a una API local de un modelo LLM (como llama3.2) utilizando Ollama. 
        Devuelve la respuesta generada por el modelo. 
        La función incluye manejo de errores en caso de que la conexión falle, la respuesta no sea JSON válido o no contenga el campo esperado.

        Creado por: | Cristian Luongo",
        Documentation.Examples = {
            [
                Description = "Ejemplo de uso: Enviar un prompt al modelo y obtener una respuesta de texto.",
                Code = "
                let
                    Prompt = ""¿Cuál es la capital de Francia?"",
                    Resultado = Call_LLM_API(Prompt)
                in
                    Resultado
                ",
                Result = "
                'París' — si el modelo está correctamente configurado y responde.
                En caso de error, se devolverá un mensaje descriptivo como:
                'Error: No se pudo conectar con la API.'"
            ]
        },
        Documentation.Category = "Text",
        Documentation.Source = "Local",
        Documentation.Author = "Cristian Luongo"
    ]
in
    Value.ReplaceType(CallLLMAPI, Documentacion)
